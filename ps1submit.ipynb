{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 3950 ps1\n",
    "**due Sun evening 11 Feb 2024 23:59**\n",
    "\n",
    "Remember to include your name and netid in the cell below. Submit via [gradescope](https://gradescope.com) -- remember to click the 'code' button to ensure that it renders properly, and it is your final saved version.\n",
    "\n",
    "<font size=\"-1\">[Also note that these problem sets are not intended as group projects: the work you submit must be your own. You can discuss with other students at a high level, for example general methods or strategies to solve a problem, but you must cite the other student(s) in your submission. Any work you submit must be your own understanding of the solution, the details of which you personally and individually worked out, and written in your own words. In no cases should notebooks or code be shared.]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "name: Kate Li\n",
    "\n",
    "netid: kl739"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to include your name and netid in the cell above (now, rather than waiting until later and forgetting ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports here, any number of cells\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "%matplotlib inline\n",
    "from collections import Counter, defaultdict\n",
    "from ps1data import absdata\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1"
   },
   "source": [
    "## 1) Naive Bayes text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any overall problem specific code here\n",
    "# in all cases below, add as many code or markup cells as you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1A"
   },
   "source": [
    "# A. two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any overall code for setting up part A, any number of cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.i) test score \n",
    "Train on the first 900 documents in each of the cs.HC and cs.LG abstracts (a total of 1800 documents), and test on the last 100 in each of the classes (a total of 200 documents). What is the test score: i.e., on the 200 test documents, what is the percentage predicted correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on the first 900 docs in each of the cs.HS and cs.LG abstracts and test on the last 100 docs\n",
    "HC = absdata['cs.HC']\n",
    "LG = absdata['cs.LG']\n",
    "\n",
    "hc_train, hc_test = HC[:900], HC[900:]\n",
    "lg_train, lg_test = LG[:900], LG[900:]\n",
    "\n",
    "train_set = hc_train + lg_train\n",
    "test_set = hc_test + lg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary dictionaries while smoothing to avoid zeroes\n",
    "HCvocab = defaultdict(lambda: 0.5)\n",
    "LGvocab = defaultdict(lambda: 0.5)\n",
    "\n",
    "# Helper function to update word counts\n",
    "def update_vocab(vocab, documents):\n",
    "    for doc in documents:\n",
    "        words = set(doc.split())\n",
    "        for w in words:\n",
    "            vocab[w] += 1\n",
    "\n",
    "update_vocab(HCvocab, hc_train)\n",
    "update_vocab(LGvocab, lg_train)\n",
    "\n",
    "# Convert the counts to probabilities\n",
    "total_hc = sum(HCvocab.values())\n",
    "total_lg = sum(LGvocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes classifier with log probabilities to avoid underestimating probabilities\n",
    "def nb_classifier(document):\n",
    "    doc_words = set(document.split())\n",
    "    hcprob = 0\n",
    "    lgprob = 0\n",
    "    \n",
    "    for w in doc_words:\n",
    "        hcprob += np.log(HCvocab[w] / total_hc)\n",
    "        lgprob += np.log(LGvocab[w] / total_lg)\n",
    "    \n",
    "    if hcprob > lgprob:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def test_classifier(train_doc, test_doc, train_labels, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train = vectorizer.fit_transform(train_doc)\n",
    "    test = vectorizer.transform(test_doc)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set (100 from HC and 100 from LG)\n",
    "test_labels = [0]*100 + [1]*100\n",
    "predictions = [nb_classifier(doc) for doc in test_set]\n",
    "print(len(predictions))\n",
    "print(test_labels[:5])\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    }
   ],
   "source": [
    "# Determine how many predictions were correct\n",
    "correct_predictions = np.sum(np.array(test_labels) == np.array(predictions))\n",
    "print(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.5%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy or test score\n",
    "accuracy = correct_predictions / len(predictions) * 100\n",
    "print(f\"{accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Aii"
   },
   "source": [
    "## A.ii) most common words\n",
    "In class (lec5), the importance of \"feature set selection\" will be mentioned. Instead of using the full vocabulary, try using just the 500 most common words (highest percentage of documents) from each of the two categories, for a total of somewhat under 1000 words (due to overlaps between the two lists). How does that affect the test score? (note that the full vocabulary for the two classes consisted of close to 14,000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in HC: 149463\n",
      "Total words in LG: 152663\n"
     ]
    }
   ],
   "source": [
    "# Count the words in HC and LG\n",
    "hc_words = []\n",
    "for text in HC:\n",
    "    for word in re.findall(r\"[a-z0-9']+\", text.lower()):\n",
    "        hc_words.append(word)\n",
    "print(f\"Total words in HC: {len(hc_words)}\")\n",
    "\n",
    "lg_words = []\n",
    "for text in LG:\n",
    "    for word in re.findall(r\"[a-z0-9']+\", text.lower()):\n",
    "        lg_words.append(word)\n",
    "print(f\"Total words in LG: {len(lg_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "734\n"
     ]
    }
   ],
   "source": [
    "# 500 most common words from HC\n",
    "hc_500 = Counter(hc_words).most_common(500)   \n",
    "print(len(hc_500))\n",
    "\n",
    "# 500 most common words from LG\n",
    "lg_500 = Counter(lg_words).most_common(500)\n",
    "print(len(lg_500))\n",
    "\n",
    "# total most common words from HC and LG\n",
    "total = hc_500 + lg_500\n",
    "set_mc = set(word for word, _ in total)\n",
    "total_mc = list(set_mc)\n",
    "print(len(total_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.52%\n"
     ]
    }
   ],
   "source": [
    "# Create the train and test set\n",
    "train_mc, test_mc = total_mc[:550], total_mc[550:]\n",
    "\n",
    "# Classify the train set\n",
    "train_classify = [nb_classifier(mc) for mc in train_mc]\n",
    "\n",
    "# Classify the test set\n",
    "test_classify = [nb_classifier(mc) for mc in test_mc]\n",
    "\n",
    "# Labels for the test data\n",
    "word_cat = {word:0 for word, _ in hc_500}\n",
    "word_cat.update({word:1 for word, _ in lg_500})\n",
    "test_labels = [word_cat[word] for word in test_mc]\n",
    "\n",
    "# Calculate the accuracy of the prediction on the test data\n",
    "correct_accuracy = np.mean(np.array(test_classify) == np.array(test_labels))\n",
    "print(f\"{round(correct_accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out the 500 most common words from both HC and LG, I determined that the test score/accuracy is lower than when we trained on the first 900 documents and tested on the last 100 documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Aiii"
   },
   "source": [
    "## A.iii) most discriminating words\n",
    "The above feature set, of 1000 most frequent terms, might not be optimal for this classification task, since many of those terms (the, of, and, or, ...) might not discriminate systematically between the two classes. They could just add noise and have an adverse effect on classifier performance. Instead we can try to use the terms that are most discriminating, in the sense of having the largest disparities in numbers of occurrences between the two classes (as will be illustrated in lec5 for the biology/physics classifier).\n",
    "\n",
    "First list the top 20 terms most discriminating in the HC direction (highest ratio (.5 + #HCtexts with word)/(.5 + #LGtexts with word)),\n",
    " and the top 20 most discriminating in the LG direction (highest reciprocal of above ratio).\n",
    "\n",
    "Then construct a new feature set consisting of just the 200 most discriminating terms in each direction and which occur in at least 11 of the 1800 texts\n",
    " (a total of 400 since there won't be overlap). What are the 20 most discriminating terms in this set?\n",
    "\n",
    "How does the test score of the classifier on the 200 test documents compare with parts i, ii)?\n",
    "\n",
    "The .5 is again \"smoothing\", to avoid division by zero for words that occur in only one of the two classes.\n",
    "\n",
    "This is so that excessive bias isn't given to terms that happen to occur very few times in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of HCtexts with word: 36\n",
      "# of LGtexts with word: 2072\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of times word appears in HC and LG\n",
    "hctexts = defaultdict(int)\n",
    "for word in hc_words:\n",
    "    hctexts[word] += 1\n",
    "print(f\"# of HCtexts with word: {hctexts[hc_words[0]]}\")\n",
    "\n",
    "lgtexts = defaultdict(int)\n",
    "for word in lg_words:\n",
    "    lgtexts[word] += 1\n",
    "print(f\"# of LGtexts with word: {lgtexts[lg_words[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HC direction ratio for the first word in hctexts: 4.867\n",
      "HC direction ratio for the first word in hctexts: 4.867\n"
     ]
    }
   ],
   "source": [
    "# Calculate the ratio of each word in the HC direction\n",
    "hcr_dict = defaultdict(float)\n",
    "for word in hctexts:\n",
    "    hcr = (0.5 + hctexts[word]) / (0.5 + lgtexts[word])\n",
    "    hcr_dict[word] += round(hcr, 3)\n",
    "print(f\"HC direction ratio for the first word in hctexts: {hcr_dict[hc_words[0]]}\")\n",
    "\n",
    "# Check work\n",
    "hc_first = hc_words[0]\n",
    "first_freq_hc = hctexts[hc_first]\n",
    "lg_match = lgtexts[hc_first]\n",
    "hcr_check = (0.5 + first_freq_hc) / (0.5 + lg_match)\n",
    "print(f\"HC direction ratio for the first word in hctexts: {round(hcr_check, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LG direction ratio for the first word in lgtexts: 0.987\n",
      "LG direction ratio for the first word in lgtexts: 0.987\n"
     ]
    }
   ],
   "source": [
    "# Calculate the ratio of each word in the LG direction\n",
    "lgr_dict = defaultdict(float)\n",
    "for word in lgtexts:\n",
    "    lgr = (0.5 + lgtexts[word]) / (0.5 + hctexts[word])\n",
    "    lgr_dict[word] += round(lgr, 3)\n",
    "print(f\"LG direction ratio for the first word in lgtexts: {lgr_dict[lg_words[0]]}\")\n",
    "\n",
    "# Check work\n",
    "lg_first = lg_words[0]\n",
    "first_freq_lg = lgtexts[lg_first]\n",
    "hc_match = hctexts[lg_first]\n",
    "lgr_check = (0.5 + first_freq_lg) / (0.5 + hc_match)\n",
    "print(f\"LG direction ratio for the first word in lgtexts: {round(lgr_check, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vr', 367.0), ('interviews', 209.0), ('hci', 207.0), ('interfaces', 197.0), ('creative', 195.0), ('immersive', 173.0), ('genai', 165.0), ('haptic', 161.0), ('visualizations', 161.0), ('perception', 137.0), ('accessibility', 137.0), ('tactile', 129.0), ('blind', 129.0), ('wearable', 127.0), ('chatbot', 125.0), ('ui', 125.0), ('perceptions', 107.0), ('chatbots', 105.0), ('ethical', 99.0), ('educational', 95.0)]\n",
      "[('federated', 195.0), ('anomaly', 145.0), ('fl', 139.0), ('kernel', 99.0), ('regularization', 97.0), ('inverse', 97.0), ('gradient', 94.333), ('tokens', 87.0), ('bounds', 87.0), ('quantum', 87.0), ('convex', 85.0), ('descent', 83.0), ('lora', 83.0), ('dependencies', 83.0), ('molecular', 83.0), ('updates', 81.0), ('guarantees', 75.0), ('generalization', 73.667), ('tensor', 71.0), ('variational', 71.0)]\n",
      "0.395\n"
     ]
    }
   ],
   "source": [
    "# List the top 20 terms most discriminating in the HC direction\n",
    "sorted_hcr = sorted(hcr_dict.items(), key = lambda item:item[1], reverse = True)\n",
    "print(sorted_hcr[:20])\n",
    "\n",
    "# List the top 20 terms most discriminating in the LG direction\n",
    "sorted_lgr = sorted(lgr_dict.items(), key = lambda item: item[1], reverse = True)\n",
    "print(sorted_lgr[:20])\n",
    "print(lgr_dict['interpret'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "['vr', 'interviews', 'hci', 'interfaces', 'creative', 'federated', 'immersive', 'genai', 'haptic', 'visualizations', 'anomaly', 'fl', 'accessibility', 'perception', 'blind', 'tactile', 'wearable', 'chatbot', 'ui', 'perceptions']\n"
     ]
    }
   ],
   "source": [
    "# 200 most discriminating words in the HC direction\n",
    "hcr_200 = sorted_hcr[:200]\n",
    "\n",
    "# 200 most disriminating words in the LG direction\n",
    "lgr_200 = sorted_lgr[:200]\n",
    "\n",
    "# Combine the list of the 200 most discriminating words from both HC and LG\n",
    "total_discrim = list(set(hcr_200 + lgr_200))\n",
    "\n",
    "## Sort total_discrim from most discriminating to least discriminating\n",
    "sorted_discrim = sorted(total_discrim, key = lambda item: item[1], reverse = True)\n",
    "\n",
    "## Keep only the words from sorted_discrim\n",
    "discriminating = [word for word, _ in sorted_discrim]\n",
    "print(len(discriminating))\n",
    "\n",
    "# Check to make sure that the words in `discriminating` appear in at least 11 texts\n",
    "filtered_words = []\n",
    "for word in discriminating:\n",
    "    if hctexts.get(word, 0) + lgtexts.get(word, 0) >= 11:\n",
    "        filtered_words.append(word)\n",
    "print(len(filtered_words))\n",
    "\n",
    "# Filter the top 20 most discriminating terms in this new set\n",
    "most_discrim = discriminating[:20]\n",
    "print(most_discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "100\n",
      "82.0%\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "train_discrim, test_discrim = discriminating[:300], discriminating[300:]\n",
    "\n",
    "# Classify the train data\n",
    "train_classify = [nb_classifier(doc) for doc in train_discrim]\n",
    "print(len(train_classify))\n",
    "\n",
    "# Classify the training data\n",
    "test_classify = [nb_classifier(doc) for doc in test_discrim]\n",
    "print(len(test_classify))\n",
    "\n",
    "# True labels for the test data\n",
    "word_cat = {word:0 for word, _ in hcr_200}\n",
    "word_cat.update({word:1 for word, _ in lgr_200})\n",
    "labels_discrim = [word_cat[word] for word in test_discrim]\n",
    "\n",
    "# Determine the accuracy of the predictions\n",
    "correct_discrim_pred = np.sum(np.array(test_classify) == np.array(labels_discrim))\n",
    "accuracy = correct_discrim_pred / len(test_classify) * 100\n",
    "print(f\"{round(accuracy, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test score on the 200 test documents from the most discriminating words is higher than the test score of the test documents of the most common words but lower than the test score of a regular train/test with all of the documents (1800 total)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1B"
   },
   "source": [
    "# B. four classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any overall code for setting up part B, any number of cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it on the first 900 abstracts in each of those four categories (a total of 3600 documents), and test on the last 100 from each of those four categories (a total of 400 documents). As features, use the full vocabulary as in A.i above (rather than the more restricted feature sets in A.ii,iii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and test sets\n",
    "HC = absdata['cs.HC']\n",
    "LG = absdata['cs.LG']\n",
    "CV = absdata['cs.CV']\n",
    "NC = absdata['q-bio.NC']\n",
    "\n",
    "train_hc, test_hc = HC[:900], HC[900:]\n",
    "train_lg, test_lg = LG[:900], LG[900:]\n",
    "train_cv, test_cv = CV[:900], CV[900:]\n",
    "train_nc, test_nc = NC[:900], NC[900:]\n",
    "\n",
    "train_totals = train_hc + train_lg + train_cv + train_nc\n",
    "test_totals = test_hc + test_lg + test_cv + test_nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the four-way naive bayes classifier where the predicted classification has the highest probability\n",
    "def four_way_classifier(train_doc, test_doc, train_labels, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train = vectorizer.fit_transform(train_doc)\n",
    "    test = vectorizer.transform(test_doc)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(train)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Generate the train and test labels while making sure they match the size of the train and test sets\n",
    "train_labels = [0]*900 + [1]*900 + [2]*900 + [3]*900\n",
    "test_labels = [0]*100 + [1]*100 + [2]*100 + [3]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Bi"
   },
   "source": [
    "## B.i) train score\n",
    "For each of the four categories, what is the percentage of the training set classified correctly? (total correct / 900 for each in these)\n",
    "What is the combined training score? (total correct in all four categories / 3600, usually called the 'training score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "# Predict with the four way classifier on the train set\n",
    "predictions = four_way_classifier(train_totals, test_totals, train_labels, test_labels)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for HC: 97.78%\n",
      "Accuracy for LG: 96.67%\n",
      "Accuracy for CV: 97.44%\n",
      "Accuracy for NC: 97.89%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for HC train\n",
    "accuracy_hc = np.mean(predictions[:900] == 0) * 100\n",
    "print(f\"Accuracy for HC: {round(accuracy_hc, 2)}%\")\n",
    "\n",
    "# Accuracy for LG train\n",
    "accuracy_lg = np.mean(predictions[900:1800] == 1) * 100\n",
    "print(f\"Accuracy for LG: {round(accuracy_lg, 2)}%\")\n",
    "\n",
    "# Accuracy for CV train\n",
    "accuracy_cv = np.mean(predictions[1800:2700] == 2) * 100\n",
    "print(f\"Accuracy for CV: {round(accuracy_cv, 2)}%\")\n",
    "\n",
    "# Accuracy for NC train\n",
    "accuracy_nc = np.mean(predictions[2700:] == 3) * 100\n",
    "print(f\"Accuracy for NC: {round(accuracy_nc, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 97.44%\n"
     ]
    }
   ],
   "source": [
    "# Combined training score\n",
    "train_score = np.mean(predictions == train_labels) * 100\n",
    "print(f\"Train score: {round(train_score, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Bii"
   },
   "source": [
    "## B.ii) test score\n",
    "For each of the four categories, what is the percentage of the test set classified correctly? (total correct / 100 for each in this case)\n",
    "What is the combined test score? (total correct in all four categories / 400 in this case, usually called the 'test score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_way_classifier_test(train_doc, test_doc, train_labels, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train = vectorizer.fit_transform(train_doc)\n",
    "    test = vectorizer.transform(test_doc)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "# Predict with the four way classifier for tests on the test set\n",
    "predictions_test = four_way_classifier_test(train_totals, test_totals, train_labels, test_labels)\n",
    "print(len(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for HC test: 92.0%\n",
      "Accuracy for LG test: 97.0%\n",
      "Accuracy for CV test: 83.0%\n",
      "Accuracy for NC test: 98.0%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for HC test\n",
    "accuracy_hc_test = np.mean(predictions_test[:100] == 0) * 100\n",
    "print(f\"Accuracy for HC test: {round(accuracy_hc_test, 2)}%\")\n",
    "\n",
    "# Accuracy for LG test\n",
    "accuracy_lg_test = np.mean(predictions_test[100:200] == 1) * 100\n",
    "print(f\"Accuracy for LG test: {round(accuracy_lg_test, 2)}%\")\n",
    "\n",
    "# Accuracy for CV test\n",
    "accuracy_cv_test = np.mean(predictions_test[200:300] == 2) * 100\n",
    "print(f\"Accuracy for CV test: {round(accuracy_cv_test, 2)}%\")\n",
    "\n",
    "# Accuracy for NC test\n",
    "accuracy_nc_test = np.mean(predictions_test[300:] == 3) * 100\n",
    "print(f\"Accuracy for NC test: {round(accuracy_nc_test, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 92.5%\n"
     ]
    }
   ],
   "source": [
    "# Combined test score\n",
    "test_score = np.mean(predictions_test == test_labels) * 100\n",
    "print(f\"Test score: {round(test_score, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Biii"
   },
   "source": [
    "## B.iii) classifier errors\n",
    "For each of the four categories, identify (and print out) the first of the test items (i.e., smallest index in the list) that is classified incorrectly. Determine that incorrect category, and determine the five words it contains that are most indicative of the incorrect category (i.e., the five words in the abstract that are most discriminating towards the wrong category over the correct category in the sense of A.iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper function to return the incorrectly guessed category\n",
    "def category(cat_num):\n",
    "    if cat_num == 0:\n",
    "        return 'Incorrect category: HC'\n",
    "    if cat_num == 1:\n",
    "        return 'Incorrect category: LG'\n",
    "    if cat_num == 2:\n",
    "        return 'Incorrect category: CV'\n",
    "    if cat_num == 3:\n",
    "        return 'Incorrect category: NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for determining the log odds of each word being in the incorrect/correct category\n",
    "def log_odds(incorr_len, corr_len):\n",
    "    log_odds = np.log((incorr_len + 1) / (corr_len + 1))\n",
    "\n",
    "    return log_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix reordering permutes the rows and columns of a matrix to reveal meaningful visual patterns, such as blocks that represent clusters. A comprehensive collection of matrices, along with a scoring method for measuring the quality of visual patterns in these matrices, contributes to building a benchmark. This benchmark is essential for selecting or designing suitable reordering algorithms for specific tasks. In this paper, we build a matrix reordering benchmark, ReorderBench, with the goal of evaluating and improving matrix reordering techniques. This is achieved by generating a large set of representative and diverse matrices and scoring these matrices with a convolution- and entropy-based method. Our benchmark contains 2,835,000 binary matrices and 5,670,000 continuous matrices, each featuring one of four visual patterns: block, off-diagonal block, star, or band. We demonstrate the usefulness of ReorderBench through three main applications in matrix reordering: 1) evaluating different reordering algorithms, 2) creating a unified scoring model to measure the visual patterns in any matrix, and 3) developing a deep learning model for matrix reordering.\n",
      "Incorrect category: LG\n"
     ]
    }
   ],
   "source": [
    "# Steps for HC test set \n",
    "# Returns a list of items that are classified incorrectly from HC's test set (as their NB classifiers)\n",
    "num_hc_incorr = [word for word in predictions_test[:100] if word != 0]\n",
    "hc_incorr_len = len(num_hc_incorr)\n",
    "hc_corr_len = len(test_hc) - hc_incorr_len\n",
    "\n",
    "if hc_incorr_len == 0:\n",
    "    print('No incorrect classifications')\n",
    "else:\n",
    "    # Assigns the first category value of num_hc_incorr to the variable first_value_hc\n",
    "    first_value_hc = num_hc_incorr[0]\n",
    "    \n",
    "    # Gets the index of the first incorrectly classified item\n",
    "    first_index_hc = list(predictions_test[:100]).index(first_value_hc)\n",
    "    \n",
    "    # Uses the index of the first incorrectly classified item to print the first test item from the test set (test_hc)\n",
    "    first_wrong_hc = test_hc[first_index_hc]\n",
    "    print(first_wrong_hc)\n",
    "    \n",
    "    # Use the helper function `category` to determine the incorrect category of the first item\n",
    "    print(category(first_value_hc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2.197), ('and', 2.079), ('of', 2.079), ('reordering', 1.792), ('the', 1.792)]\n"
     ]
    }
   ],
   "source": [
    "# Find the discriminating score for each word in the first incorrectly classified item in HC\n",
    "# Make a counter for the frequency of each word in the incorrect and correct categories\n",
    "incorrect_count = Counter()\n",
    "correct_count = Counter()\n",
    "for word in first_wrong_hc.split():\n",
    "    if category(first_value_hc) == 0:\n",
    "        correct_count[word] += 1\n",
    "    else:\n",
    "        incorrect_count[word] += 1\n",
    "\n",
    "# Use these frequencies to create a dictionary with the log odds for each word\n",
    "log_odds_hc = {}\n",
    "for word in first_wrong_hc.split():\n",
    "    incorr = incorrect_count.get(word, 0)\n",
    "    corr = correct_count.get(word, 0)\n",
    "    log_odds_hc[word] = round(log_odds(incorr, corr), 3)\n",
    "\n",
    "# Higher log odds means the word is more discriminated towards the incorrect category\n",
    "hc_prob_ranked = sorted(log_odds_hc.items(), key = lambda x:x[1], reverse = True)\n",
    "top5_hc = hc_prob_ranked[:5]\n",
    "print(top5_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent advancements in language representation learning primarily emphasize language modeling for deriving meaningful representations, often neglecting style-specific considerations. This study addresses this gap by creating generic, sentence-level style embeddings crucial for style-centric tasks. Our approach is grounded on the premise that low-level text style changes can compose any high-level style. We hypothesize that applying this concept to representation learning enables the development of versatile text style embeddings. By fine-tuning a general-purpose text encoder using contrastive learning and standard cross-entropy loss, we aim to capture these low-level style shifts, anticipating that they offer insights applicable to high-level text styles. The outcomes prompt us to reconsider the underlying assumptions as the results do not always show that the learned style representations capture high-level text styles.\n",
      "Incorrect category: CV\n"
     ]
    }
   ],
   "source": [
    "# Steps for LG test set \n",
    "# Returns a list of items that are classified incorrectly from LG's test set (as their NB classifiers)\n",
    "num_lg_incorr = [word for word in predictions_test[100:200] if word != 1]\n",
    "lg_incorr_len = len(num_lg_incorr)\n",
    "lg_corr_len = len(test_lg) - lg_incorr_len\n",
    "\n",
    "if lg_incorr_len == 0:\n",
    "    print('No incorrect classifications')\n",
    "else:\n",
    "    # Assigns the first category value of num_lg_incorr to the variable first_value_lg\n",
    "    first_value_lg = num_lg_incorr[0]\n",
    "    \n",
    "    # Gets the index of the first incorrectly classified item \n",
    "    first_index_lg = list(predictions_test[100:200]).index(first_value_lg)\n",
    "    \n",
    "    # Uses the index of the first incorrectly classified item to print the first test item from the test set (test_lg)\n",
    "    first_wrong_lg = test_lg[first_index_lg]\n",
    "    print(first_wrong_lg)\n",
    "    \n",
    "    # Use the helper function `category` to determine the incorrect category of the first item\n",
    "    print(category(first_value_lg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('style', 1.792), ('the', 1.792), ('text', 1.792), ('that', 1.609), ('to', 1.609)]\n"
     ]
    }
   ],
   "source": [
    "# Find the discriminating score for each word in the first incorrectly classified item in LG\n",
    "# Make a counter for the frequency of each word in the incorrect and correct categories\n",
    "incorrect_count = Counter()\n",
    "correct_count = Counter()\n",
    "for word in first_wrong_lg.split():\n",
    "    if category(first_value_lg) == 0:\n",
    "        correct_count[word] += 1\n",
    "    else:\n",
    "        incorrect_count[word] += 1\n",
    "\n",
    "# Use these frequencies to create a dictionary with the log odds for each word\n",
    "log_odds_lg = {}\n",
    "for word in first_wrong_lg.split():\n",
    "    incorr = incorrect_count.get(word, 0)\n",
    "    corr = correct_count.get(word, 0)\n",
    "    log_odds_lg[word] = round(log_odds(incorr, corr), 3)\n",
    "\n",
    "# Higher log odds means the word is more discriminated towards the incorrect category\n",
    "lg_prob_ranked = sorted(log_odds_lg.items(), key = lambda x:x[1], reverse = True)\n",
    "top5_lg = lg_prob_ranked[:5]\n",
    "print(top5_lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current deep learning powered appearance based uncertainty-aware gaze estimation models produce inconsistent and unreliable uncertainty estimation that limits their adoptions in downstream applications. In this study, we propose a workflow to improve the accuracy of uncertainty estimation using probability calibration with a few post hoc samples. The probability calibration process employs a simple secondary regression model to compensate for inaccuracies in estimated uncertainties from the deep learning model. Training of the secondary model is detached from the main deep learning model and thus no expensive weight tuning is required. The added calibration process is lightweight and relatively independent from the deep learning process, making it fast to run and easy to implement. We evaluated the effectiveness of the calibration process under four potential application scenarios with two datasets that have distinctive image characteristics due to the data collection setups. The calibration process is most effective when the calibration and testing data share similar characteristics. Even under suboptimal circumstances that calibration and testing data differ, the calibration process can still make corrections to reduce prediction errors in uncertainty estimates made by uncalibrated models.\n",
      "Incorrect category: LG\n"
     ]
    }
   ],
   "source": [
    "# Steps for CV test set \n",
    "# Returns a list of items that are classified incorrectly from CV's test set (as their NB classifiers)\n",
    "num_cv_incorr = [word for word in predictions_test[200:300] if word != 2]\n",
    "cv_incorr_len = len(num_cv_incorr)\n",
    "cv_corr_len = len(test_cv) - cv_incorr_len\n",
    "\n",
    "if cv_incorr_len == 0:\n",
    "    print('No incorrect classifications')\n",
    "else:\n",
    "    # Assigns the first category value of num_cv_incorr to the variable first_value_cv\n",
    "    first_value_cv = num_cv_incorr[0]\n",
    "    \n",
    "    # Gets the index of the first incorrectly classified item \n",
    "    first_index_cv = list(predictions_test[200:300]).index(first_value_cv)\n",
    "    \n",
    "    # Uses the index of the first incorrectly classified item to print the first test item from the test set (test_cv)\n",
    "    first_wrong_cv = test_cv[first_index_cv]\n",
    "    print(first_wrong_cv)\n",
    "    \n",
    "    # Use the helper function `category` to determine the incorrect category of the first item\n",
    "    print(category(first_value_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2.398), ('calibration', 2.197), ('and', 1.946), ('to', 1.946), ('process', 1.792)]\n"
     ]
    }
   ],
   "source": [
    "# Find the discriminating score for each word in the first incorrectly classified item in CV\n",
    "# Make a counter for the frequency of each word in the incorrect and correct categories\n",
    "incorrect_count = Counter()\n",
    "correct_count = Counter()\n",
    "for word in first_wrong_cv.split():\n",
    "    if category(first_value_cv) == 0:\n",
    "        correct_count[word] += 1\n",
    "    else:\n",
    "        incorrect_count[word] += 1\n",
    "\n",
    "# Use these frequencies to create a dictionary with the log odds for each word\n",
    "log_odds_cv = {}\n",
    "for word in first_wrong_cv.split():\n",
    "    incorr = incorrect_count.get(word, 0)\n",
    "    corr = correct_count.get(word, 0)\n",
    "    log_odds_cv[word] = round(log_odds(incorr, corr), 3)\n",
    "\n",
    "# Higher log odds means the word is more discriminated towards the incorrect category\n",
    "cv_prob_ranked = sorted(log_odds_cv.items(), key = lambda x:x[1], reverse = True)\n",
    "top5_cv = cv_prob_ranked[:5]\n",
    "print(top5_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animals move in three dimensions (3D). Thus, 3D measurement is necessary to report the true kinematics of animal movement. Existing 3D measurement techniques draw on specialized hardware, such as motion capture or depth cameras, as well as deep multi-view and monocular computer vision. Continued advances at the intersection of deep learning and computer vision will facilitate 3D tracking across more anatomical features, with less training data, in additional species, and within more natural, occlusive environments. 3D behavioral measurement enables unique applications in phenotyping, investigating the neural basis of behavior, and designing artificial agents capable of imitating animal behavior.\n",
      "Incorrect category: CV\n"
     ]
    }
   ],
   "source": [
    "# Steps for NC test set \n",
    "# Returns a list of items that are classified incorrectly from NC's test set (as their NB classifiers)\n",
    "num_nc_incorr = [word for word in predictions_test[300:400] if word != 3]\n",
    "nc_incorr_len = len(num_nc_incorr)\n",
    "nc_corr_len = len(test_nc) - nc_incorr_len\n",
    "\n",
    "if nc_incorr_len == 0:\n",
    "    print('No incorrect classifications')\n",
    "else:\n",
    "    # Assigns the first category value of num_nc_incorr to the variable first_value_nc\n",
    "    first_value_nc = num_nc_incorr[0]\n",
    "    \n",
    "    # Gets the index of the first incorrectly classified item \n",
    "    first_index_nc = list(predictions_test[300:400]).index(first_value_nc)\n",
    "    \n",
    "    # Uses the index of the first incorrectly classified item to print the first test item from the test set (test_nc)\n",
    "    first_wrong_nc = test_nc[first_index_nc]\n",
    "    print(first_wrong_nc)\n",
    "    \n",
    "    # Use the helper function `category` to determine the incorrect category of the first item\n",
    "    print(category(first_value_nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('3D', 1.609), ('of', 1.609), ('and', 1.609), ('in', 1.386), ('measurement', 1.386)]\n"
     ]
    }
   ],
   "source": [
    "# Find the discriminating score for each word in the first incorrectly classified item in NC\n",
    "# Make a counter for the frequency of each word in the incorrect and correct categories\n",
    "incorrect_count = Counter()\n",
    "correct_count = Counter()\n",
    "for word in first_wrong_nc.split():\n",
    "    if category(first_value_nc) == 0:\n",
    "        correct_count[word] += 1\n",
    "    else:\n",
    "        incorrect_count[word] += 1\n",
    "\n",
    "# Use these frequencies to create a dictionary with the log odds for each word\n",
    "log_odds_nc = {}\n",
    "for word in first_wrong_nc.split():\n",
    "    incorr = incorrect_count.get(word, 0)\n",
    "    corr = correct_count.get(word, 0)\n",
    "    log_odds_nc[word] = round(log_odds(incorr, corr), 3)\n",
    "\n",
    "# Higher log odds means the word is more discriminated towards the incorrect category\n",
    "nc_prob_ranked = sorted(log_odds_nc.items(), key = lambda x:x[1], reverse = True)\n",
    "top5_nc = nc_prob_ranked[:5]\n",
    "print(top5_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1C"
   },
   "source": [
    "# C.  twelve classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any overall code for setting up part C, any number of cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a twelve way classifier (all twelve classes in cell [1] above), trained on the first 900 abstracts in each of the categories (a total of 10800 documents), and test on the last 100 from each of those twelve categories (a total of 1200 documents), again using the full vocabulary as in A.i above as features.\n",
    "\n",
    "Your code should not be a pasted/edited 12 repetitions of the code from part A -- instead you should use loops over the twelve classnames both to accumulate the word occurrences for the classes at train time, and in the classify function you build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that make predictions for the train and test sets\n",
    "# Helper function for preparing labels for the train and test set to compare accuracies\n",
    "def prepare_labels(abstracts, train_size, test_size):\n",
    "    train_data, train_labels, test_data, test_labels = [], [], [], []\n",
    "\n",
    "    for label, docs in abstracts.items():\n",
    "        train_data.extend(docs[:train_size])\n",
    "        train_labels.extend([label] * train_size)\n",
    "        test_data.extend(docs[-test_size:])\n",
    "        test_labels.extend([label] * test_size)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Helper functions for classifying the train set docs and test set docs\n",
    "# Training the model based on the train set and predicting on the train set \n",
    "def twelve_classifier_train(train_docs, train_labels, test_docs, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train = vectorizer.fit_transform(train_docs)\n",
    "    test = vectorizer.transform(test_docs)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(train)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Training the model based on the train set and predicting on the test set \n",
    "def twelve_classifier_test(train_docs, train_labels, test_docs, test_labels):\n",
    "    vectorizer = CountVectorizer()\n",
    "    train = vectorizer.fit_transform(train_docs)\n",
    "    test = vectorizer.transform(test_docs)\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "problem": "1Ci"
   },
   "source": [
    "## C.i) train score\n",
    "For each of the 12 categories, what is the percentage of the training set classified correctly? (total correct / 900 for each in these)\n",
    "What is the combined training score? (total correct in all 12 categories / 10800, usually called the 'training score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of train_astro correct: 93.33%\n",
      "Percent of train_mes_hall correct: 92.22%\n",
      "Percent of train_cv correct: 97.33%\n",
      "Percent of train_hc correct: 97.0%\n",
      "Percent of train_lg correct: 95.0%\n",
      "Percent of train_hep_ph correct: 94.67%\n",
      "Percent of train_hep_th correct: 97.56%\n",
      "Percent of train_math correct: 98.0%\n",
      "Percent of train_physics_app correct: 89.33%\n",
      "Percent of train_physics_comp correct: 90.44%\n",
      "Percent of train_nc correct: 97.44%\n",
      "Percent of train_quant correct: 91.11%\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions made on the train sets\n",
    "train_data, train_labels, test_data, test_labels = prepare_labels(absdata, 900, 100)\n",
    "pred = twelve_classifier_train(train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "# Calculate the percentage of each training set classified correctly\n",
    "# Accuracy of predictions on train_astro\n",
    "accuracy_astro = np.mean(pred[:900] == train_labels[:900]) * 100\n",
    "print(f\"Percent of train_astro correct: {round(accuracy_astro, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_mes_hall\n",
    "accuracy_mes_hall = np.mean(pred[900:1800] == train_labels[900:1800]) * 100\n",
    "print(f\"Percent of train_mes_hall correct: {round(accuracy_mes_hall, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_cv\n",
    "accuracy_cv = np.mean(pred[1800:2700] == train_labels[1800:2700]) * 100\n",
    "print(f\"Percent of train_cv correct: {round(accuracy_cv, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_hc\n",
    "accuracy_hc = np.mean(pred[2700:3600] == train_labels[2700:3600]) * 100\n",
    "print(f\"Percent of train_hc correct: {round(accuracy_hc, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_lg\n",
    "accuracy_lg = np.mean(pred[3600:4500] == train_labels[3600:4500]) * 100\n",
    "print(f\"Percent of train_lg correct: {round(accuracy_lg, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_hep_ph\n",
    "accuracy_hep_ph = np.mean(pred[4500:5400] == train_labels[4500:5400]) * 100\n",
    "print(f\"Percent of train_hep_ph correct: {round(accuracy_hep_ph, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_hep_th\n",
    "accuracy_hep_th = np.mean(pred[5400:6300] == train_labels[5400:6300]) * 100\n",
    "print(f\"Percent of train_hep_th correct: {round(accuracy_hep_th, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_math\n",
    "accuracy_math = np.mean(pred[6300:7200] == train_labels[6300:7200]) * 100\n",
    "print(f\"Percent of train_math correct: {round(accuracy_math, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_physics_app\n",
    "accuracy_physics_app = np.mean(pred[7200:8100] == train_labels[7200:8100]) * 100\n",
    "print(f\"Percent of train_physics_app correct: {round(accuracy_physics_app, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_physics_comp\n",
    "accuracy_physics_comp = np.mean(pred[8100:9000] == train_labels[8100:9000]) * 100\n",
    "print(f\"Percent of train_physics_comp correct: {round(accuracy_physics_comp, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_nc\n",
    "accuracy_nc = np.mean(pred[9000:9900] == train_labels[9000:9900]) * 100\n",
    "print(f\"Percent of train_nc correct: {round(accuracy_nc, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on train_quant\n",
    "accuracy_quant = np.mean(pred[9900:10800] == train_labels[9900:10800]) * 100\n",
    "print(f\"Percent of train_quant correct: {round(accuracy_quant, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 94.45%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the combined training score\n",
    "train_score = np.mean(pred == train_labels) * 100\n",
    "print(f\"Train score: {round(train_score, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Cii"
   },
   "source": [
    "## C.ii) test score\n",
    "For each of the twelve categories, what is the percentage of the test set classified correctly? (total correct / 100 for each in this case)\n",
    "What is the combined test score? (total correct in all four categories / 1200 in this case, usually called the 'test score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of test_astro correct: 85.0%\n",
      "Percent of test_mes_hall correct: 81.0%\n",
      "Percent of test_cv correct: 82.0%\n",
      "Percent of test_hc correct: 90.0%\n",
      "Percent of test_lg correct: 97.0%\n",
      "Percent of test_hep_ph correct: 89.0%\n",
      "Percent of test_hep_th correct: 94.0%\n",
      "Percent of test_math correct: 92.0%\n",
      "Percent of test_physics_app correct: 74.0%\n",
      "Percent of test_physics_comp correct: 82.0%\n",
      "Percent of test_nc correct: 96.0%\n",
      "Percent of test_quant correct: 81.0%\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions made on the test sets\n",
    "train_data, train_labels, test_data, test_labels = prepare_labels(absdata, 900, 100)\n",
    "pred = twelve_classifier_test(train_data, train_labels, test_data, test_labels)\n",
    "\n",
    "# Calculate the percentage of each test set classified correctly\n",
    "# Accuracy of predictions on test_astro\n",
    "accuracy_astro = np.mean(pred[:100] == test_labels[:100]) * 100\n",
    "print(f\"Percent of test_astro correct: {round(accuracy_astro, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_mes_hall\n",
    "accuracy_mes_hall = np.mean(pred[100:200] == test_labels[100:200]) * 100\n",
    "print(f\"Percent of test_mes_hall correct: {round(accuracy_mes_hall, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_cv\n",
    "accuracy_cv = np.mean(pred[200:300] == test_labels[200:300]) * 100\n",
    "print(f\"Percent of test_cv correct: {round(accuracy_cv, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_hc\n",
    "accuracy_hc = np.mean(pred[300:400] == test_labels[300:400]) * 100\n",
    "print(f\"Percent of test_hc correct: {round(accuracy_hc, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_lg\n",
    "accuracy_lg = np.mean(pred[400:500] == test_labels[400:500]) * 100\n",
    "print(f\"Percent of test_lg correct: {round(accuracy_lg, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_hep_ph\n",
    "accuracy_hep_ph = np.mean(pred[500:600] == test_labels[500:600]) * 100\n",
    "print(f\"Percent of test_hep_ph correct: {round(accuracy_hep_ph, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_hep_th\n",
    "accuracy_hep_th = np.mean(pred[600:700] == test_labels[600:700]) * 100\n",
    "print(f\"Percent of test_hep_th correct: {round(accuracy_hep_th, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_math\n",
    "accuracy_math = np.mean(pred[700:800] == test_labels[700:800]) * 100\n",
    "print(f\"Percent of test_math correct: {round(accuracy_math, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_physics_app\n",
    "accuracy_physics_app = np.mean(pred[800:900] == test_labels[800:900]) * 100\n",
    "print(f\"Percent of test_physics_app correct: {round(accuracy_physics_app, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_physics_comp\n",
    "accuracy_physics_comp = np.mean(pred[900:1000] == test_labels[900:1000]) * 100\n",
    "print(f\"Percent of test_physics_comp correct: {round(accuracy_physics_comp, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_nc\n",
    "accuracy_nc = np.mean(pred[1000:1100] == test_labels[1000:1100]) * 100\n",
    "print(f\"Percent of test_nc correct: {round(accuracy_nc, 2)}%\")\n",
    "\n",
    "# Accuracy of predictions on test_quant\n",
    "accuracy_quant = np.mean(pred[1100:1200] == test_labels[1100:1200]) * 100\n",
    "print(f\"Percent of test_quant correct: {round(accuracy_quant, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 86.92%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the combined test score\n",
    "test_score = np.mean(pred == test_labels) * 100\n",
    "print(f\"Test score: {round(test_score, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "problem": "1Ciii"
   },
   "source": [
    "## C.iii) [bonus] classifier errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Triple-check that you've included your name and netid in the markup cell near the top of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
